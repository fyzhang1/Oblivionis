# Federated Unlearning Configuration
model:
  model_name_or_path: "facebook/opt-125m"  # Pre-trained Model Path
  cache_dir: null

tokenizer:
  tokenizer_name_or_path: "facebook/opt-125m"  # Tokenizer Path
  cache_dir: null

data:
  train_file: "data/toxic_pile/train.json"  # Training Data File
  validation_file: "data/toxic_pile/validation.json"  # Validation Data File
  test_file: "data/toxic_pile/test.json"  # Test Data File
  text_column: "text"  # Text Column Name
  preprocessing_num_workers: 4

training:
  output_dir: "output/federated_unlearning"  # Output Directory
  seed: 42
  do_train: true
  do_eval: true
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 1
  learning_rate: 2.0e-5
  weight_decay: 0.01
  num_train_epochs: 3
  warmup_ratio: 0.1
  logging_steps: 50
  save_total_limit: 3
  save_steps: 100
  evaluation_strategy: "steps"
  eval_steps: 500
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  fp16: true

unlearning:
  forget_file: "data/toxic_pile/forget.json"  # Data to Be Forgotten
  retain_file: "data/toxic_pile/retain.json"  # Data to Be Retained
  unlearning_method: "FederatedUnlearning"  # Unlearning Method

# Federated Learning Specific Configuration
federated:
  num_clients: 5  # Number of Clients
  target_client_idx: 0  # Target Client Index
  client_epochs: 2  # Number of Training Epochs per Client
  aggregation_strategy: "fedavg"  # Aggregation Strategy: FedAvg, Weighted

# Privacy and Security Configuration
privacy:
  enable_differential_privacy: false  # Whether to Enable Differential Privacy
  noise_multiplier: 1.0  # Noise Multiplier
  max_grad_norm: 1.0  # Gradient Clipping Norm

# Evaluation Configuration
evaluation:
  membership_inference: true  # Whether to Perform Membership Inference Attack Evaluation
  privacy_risk: true  # Whether to Evaluate Privacy Risk
  utility_metrics: ["perplexity", "accuracy"]  # Utility Metrics