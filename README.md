<div align="center">
<h1><strong>ğŸŒ“ Federated LLM Unlearning</strong></h2>
</div>


---------
## ğŸ”¥ Introduction
We aim to construct the Federated LLM Unlearning: A unified framework for building secure federated large models that are trainable and forgettable on local private data.

- Multiple federated learning algorithms
- Multiple evaluation indicators
- Multiple unlearning and federated unlearning algorithms
---------
# ğŸ Quick Start
##  ğŸ§© LoRA Federated Finetune (TOFU)
```python
python src/federated_train.py \
--config-name=train-lora.yaml \
experiment=finetune/tofu/default \
task_name=FedProx_LoRA_Finetune_llama3b
```
`task_name={fed}_LoRA+{Finetune\Unlearn name}_{model}`

`The fine-tune model (initial global model) is saved: "saves/finetune/{task_name}"`


##  ğŸª­TOFU Unlearning

##### Unlearning: Federated Unlearning (GradAscent, Fedavg)
```python
python src/fed_train.py --config-name=unlearn.yaml experiment=unlearn/tofu/default \forget_split=forget10 retain_split=retain90 trainer=FederatedUnlearningTrainer task_name=test \
model=Llama-3.2-3B-Instruct \
model.model_args.pretrained_model_name_or_path=saves/finetune/SAMPLE_TRAIN
```
```python
The global model is saved "saves/unlearn/test" #test is the task_name
```
- config file: "configs/trainer/FederatedUnlearningTrainer"

## TOFU Eval

##### Using TOFU benchmark to evaluate federated global model

```python
CUDA_VISIBLE_DEVICES=0 python src/eval.py \
experiment=eval/tofu/default.yaml \
forget_split=forget10 \
holdout_split=retain90 \
task_name=test \
model.model_args.pretrained_model_name_or_path=saves/unlearn/test \
paths.output_dir=saves/unlearn/test/evals \
retain_logs_path=saves/eval/SAMPLE_TRAIN/TOFU_EVAL.json
```

```python
The results are saved "saves/unlearn/test/evals" 
```
- retain_logs_path: the reference of initial global's evaluation


## ğŸ”‘ MUSE Unlearning
##### MUSE: federated training (A100 80G)
```python
python src/fed_train.py --config-name=unlearn.yaml \
experiment=unlearn/muse/default.yaml \
model=Llama-2-7b-hf \
data_split=News \
trainer=FederatedUnlearningTrainer \
task_name=test \
retain_logs_path=saves/eval/muse_Llama-2-7b-hf_News_retrain/MUSE_EVAL.json \
trainer.args.per_device_train_batch_size=2 \
trainer.args.gradient_accumulation_steps=8
```

## Â ğŸš€ Support

### ğŸ”¥â„ï¸ Lora

#### 1.model config

`configs/model/Llama-3.2-3B-Instruct-lora.yaml`  or create new file

```yaml
model_args:
  pretrained_model_name_or_path: "meta-llama/Llama-3.2-3B-Instruct"
  attn_implementation: 'flash_attention_2'
  torch_dtype: bfloat16
  use_lora: true
  lora_config:
    r: 32                    # LoRA rank
    lora_alpha: 64          # LoRA alphaå‚æ•°
    target_modules: ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    lora_dropout: 0.05       # LoRA dropoutç‡
    bias: "none"            # biaså¤„ç†æ–¹å¼
```



#### 2.training config

`configs/unlearn-lora.yaml`:

```yaml
defaults:
  - model: Llama-3.2-3B-Instruct-lora
  - trainer: FederatedUnlearningTrainer
  - data: unlearn
  - collator: DataCollatorForSupervisedDataset
  - eval: tofu
  - hydra: default
  - paths: default
  - experiment: null
  - _self_

trainer:
  args: 
    remove_unused_columns: False

mode: unlearn
task_name: ???
```



#### Start Lora

```bash
python src/fed_train.py \
  --config-name=unlearn-lora.yaml \
  experiment=unlearn/tofu/default \
  forget_split=forget10 \
  retain_split=retain90 \
  trainer=FederatedUnlearningTrainer \
  task_name=fed_lora_unlearn \
  model=Llama-3.2-3B-Instruct-lora \ #1B/3B/7B
  model.model_args.pretrained_model_name_or_path=saves/finetune/SAMPLE_TRAIN #change
```



## â­ï¸ Acknowledgements

- This repo is inspired from [OpenUnlearning](https://github.com/locuslab/open-unlearning). 

---------------------------


"""
   configs/
   â”œâ”€â”€ unlearn.yaml              # åŸºç¡€é…ç½®
   â”œâ”€â”€ experiment/
   â”‚   â””â”€â”€ unlearn/
   â”‚       â””â”€â”€ tofu/
   â”‚           â””â”€â”€ default.yaml  # å®éªŒé…ç½®
   â”œâ”€â”€ model/                    # æ¨¡å‹é…ç½®
   â”œâ”€â”€ trainer/                  # è®­ç»ƒå™¨é…ç½®
   â”œâ”€â”€ data/                    # æ•°æ®é…ç½®
   â””â”€â”€ eval/                    # è¯„ä¼°é…ç½®
"""


"""
Hydraé…ç½®ç³»ç»Ÿ:è¿™ä¸ªè£…é¥°å™¨å‘Šè¯‰Hydra: 1.é…ç½®æ–‡ä»¶åœ¨../configsç›®å½•ä¸‹ 2.é»˜è®¤ä½¿ç”¨train.yamlä½œä¸ºåŸºç¡€é…ç½®æ–‡ä»¶
Hydraä¼šè‡ªåŠ¨å°†è¿™äº›å‘½ä»¤è¡Œå‚æ•°è½¬æ¢æˆé…ç½®å¯¹è±¡, è¿™äº›å‚æ•°ä¼šè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤å€¼, æ‰€æœ‰è¿™äº›é…ç½®æœ€ç»ˆä¼šè¢«åˆå¹¶åˆ°ä¼ é€’ç»™mainå‡½æ•°çš„cfgå‚æ•°ä¸­
hydraä¼šå°†æœ€ç»ˆçš„é…ç½®ä½œä¸ºDictConfigå¯¹è±¡ä¼ é€’ç»™mainå‡½æ•°: cfg
"""


"""
åšè”é‚¦ç»Ÿä¸€æ¡†æ¶çš„å¤§æ¦‚é€»è¾‘:
1.è®¾ç½®3ä¸ªå®¢æˆ·ç«¯è¿›è¡Œfine-tune, æ•°æ®è¿›è¡Œåˆ‡å‰²/æˆ–è€…ä¸è¿›è¡Œè¿™ä¸€æ­¥, å‡è®¾ä¸‹è½½çš„é›†ä¸­å¼modelå°±æ˜¯è”é‚¦è®­ç»ƒåçš„global model
2.å¾—åˆ°ä¸€ä¸ªglobal model, å¹¶å—åˆ°å…¶ä¸­ä¸€ä¸ªå®¢æˆ·ç«¯çš„é—å¿˜è¯·æ±‚
3.å®šä¹‰è¿™ä¸ªå®¢æˆ·ç«¯çš„é—å¿˜æ•°æ®, å¹¶è¿›è¡Œunlearning
4.å°†unlearningåçš„æ¨¡å‹æ›´æ–°å‘é€åˆ°æœåŠ¡å™¨ã€‚æœåŠ¡å™¨é‡æ–°èšåˆæ¨¡å‹,æ›´æ–°global model
5.åœ¨global modelä¸Šè¿›è¡Œè¯„ä¼°,éªŒè¯æ¨¡å‹æ€§èƒ½ã€‚
"""
